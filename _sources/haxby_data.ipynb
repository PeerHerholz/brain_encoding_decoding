{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfc9345",
   "metadata": {},
   "source": [
    "(haxby-dataset)=\n",
    "# An overview of the Haxby dataset\n",
    "\n",
    "This part of the `tutorial` aims to make `participants` familiar with the `dataset` we are going to use during this session and also address/introduce/recap some important aspects concerning `datasets` within `machine learning`/`decoding`. The objectives 📍 are:\n",
    "\n",
    "- (re-)familiarize everyone with important `datasets` aspects\n",
    "\n",
    "- exploring and understand the `tutorial dataset`\n",
    "\n",
    "\n",
    "## A short primer on datasets\n",
    "\n",
    "We wanted to avoid \"just talking\" about `brain decoding` in theory and also showcase how the respective `models` and workflows can be implemented, as well as run to give you some first hands-on experience. Even though we would have loved to get everyone to bring their data and directly apply the things we talk about, it's unfortunately a bit too time-consuming for this setting. Thus, we decided to utilize an `example dataset` that is ready to go and \"small enough\" to run `decoding models` locally, ie on laptops. You might think \"One of those tutorials again...it works with the example dataset but I have little or no chance on running it on/adapting it to my data.\" and we would agree based on workshops we did ourselves. \n",
    "\n",
    "However, we tried our best to address this here by utilizing `software` whose `workflows` and `processing steps` are rather agnostic and implemented via `high-level API` that _should_ allow a comparably straightforward application to different kinds of `data`. This specifically refers to a set of core aspects concerning the dataset's structure and information entailed therein. How about a brief recap?\n",
    "\n",
    "```{figure} graphics/decoding_pipeline_example.png\n",
    "---\n",
    "width: 800px\n",
    "name: decoding_pipeline_example\n",
    "---\n",
    "\n",
    "A schematic representation of standard `decoding workflow`/`pipeline`. The `input` (`data`) is prepared and potentially `preprocessed` before being submitted to a `model` that then utilizes a certain `metric` to provide a certain `output`.\n",
    "``` \n",
    "\n",
    "Here, we are going to focus on the `input` (`data`). As you heard before, it is usually expected to be structured as `samples` X `features`. \n",
    "\n",
    "```{admonition} What could samples X features refer to/entail?\n",
    ":class: tip, dropdown\n",
    "\n",
    "A `sample` could be considered an `observation`/`data point`/one distinct entity in the `dataset`/one distinct part of the `dataset`. For example, if you want to `predict` what a participant perceived based on their `brain activation`/`response`, the `samples` could entail the `fMRI` `scans` or estimated `contrast images` of that `participant`. If you want to `predict` whether a `participant` exhibited a certain `behavior`, e.g. a captured by a `clinical measure`, etc., then the `samples` could comprise different `participants`.  \n",
    "\n",
    "A `feature` on the other hand would entail/describe certain aspects of a given `sample`. For example, if you want to `predict` what a participant perceived based on their `brain activation`/`response`, the `features` could entail the `voxel pattern` at a certain `ROI` at the given `sample`. \n",
    "```\n",
    "\n",
    "Thus, in order to make a given `dataset` \"ready\" for `machine learning`/`decoding`, we need to get it into the respective structure. Lucky for us, the tools we are going to explore, specifically `nilearn`, incorporate this aspect and are make the corresponding process rather easy. What you need to run `machine learning`/`decoding` on your `dataset` is:\n",
    "\n",
    "- know what your `samples` are (e.g. `time series`, `statistical maps`, etc.)\n",
    "- know what your `features` are (e.g. `voxel pattern` of an `ROI`, `annotations`, etc.)\n",
    "- get the `dataset` in the form `samples` X `features`, ie `samples` are `rows` and `features` are `columns`\n",
    "\n",
    "While exploring the `tutorial dataset` we will refer to this to make it more clear.\n",
    "\n",
    "```{admonition} Bonus question: ever heard of the \"small-n-high-p\" (p >> n) problem?\n",
    ":class: tip, dropdown\n",
    "\n",
    "\"Classical\" `machine learning`/`decoding` models and the underlying algorithms operate on the assumption that are more `predictors` or `features` than there are `sample`. In fact many more. Why is that?\n",
    "Consider a high-dimensional `space` whose `dimensions` are defined by the number of `features` (e.g. `10 features` would result in a space with `10 dimensions`. The resulting `volume` of this `space` is the amount of `samples` that could be drawn from the `domain` and the number of `samples` entail the `samples` you need to address your `learning problem`, ie `decoding` outcome. That is why folks say: \"get more data\", `machine learning` is `data`-hungry: our `sample` needs to be as representative of the high-dimensional domain as possible. Thus, as the number of `features` increases, so should the number of `samples` so to capture enough of the `space` for the `decoding model` at hand.\n",
    "\n",
    "This referred to as the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) and poses as a major problem in many fields that aim to utilize `machine learning`/`decoding` on unsuitable data. Why is that?\n",
    "Just imagine we have way more `features` than `samples`, ie `50 features` and `10` `samples`. Instead of having a large amount of `samples` within the `space`, allowing to achieve a sufficient coverage of the latter, we now have a very high-dimensional `space` (`50 dimensions`) and only very few `samples` therein, basically not allowing us to capture nearly enough of the `space` as we would need to. This can result in expected outcomes, misleading results or even lead to complete `model` failure. Furthermore, respective `datasets` often lead to `models` that are `overfitted` and don't `generalize` well. \n",
    "\n",
    "However, there are a few things that can be done to address this, including `feature selection`, `projection` into `lower-dimensional` `spaces` or `representations` or `regularization`. \n",
    "\n",
    "Question for everyone: what kind have `datasets` do we usually have in `neuroscience`, especially `neuroimaging`?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb4d24",
   "metadata": {},
   "source": [
    "## Downloading & exploring the `Haxby dataset`\n",
    "\n",
    "In the field of `functional magnetic resonance imaging` (`fMRI`), one of the first studies which have demonstrated the feasibility of `brain decoding` was the study by Haxby and colleagues (2001) {cite:p}`Haxby2001-vt`. `Subjects` were presented with various `images` drawn from different `categories` and subsequently a `decoding model` used to `predict` the presented `categories` based on the `brain activity`/`responses`. In the respective parts of this session, we will try to do the same! \n",
    "\n",
    "We are going to start with one `subject`, number `4`. To get the `data`, we can simply use [nilearn's dataset module](https://nilearn.github.io/stable/modules/datasets.html). At first, we need to import the respective `module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d377593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nilearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ec6fd",
   "metadata": {},
   "source": [
    "Next, we get the `data` and going to save it in a directory called `data`. Depending on your machine and internet connection, this might take a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('..', 'data')\n",
    "haxby_dataset = datasets.fetch_haxby(subjects=[4], fetch_stimuli=True, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec5239",
   "metadata": {},
   "source": [
    "What do we have now? Lets have a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea85b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "haxby_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa01ea7",
   "metadata": {},
   "source": [
    "As you can see, we get a `python dictionary` and there's quite bit in it. This includes:\n",
    "\n",
    "- the `anatomical data` under `anat`\n",
    "- the `functional data` under `func`\n",
    "- an annotation when `participants` perceived what `category`\n",
    "- several `masks` under `mask*`\n",
    "- a `dataset` `description`\n",
    "- `stimuli categories` and respective `stimuli`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93da5d2",
   "metadata": {},
   "source": [
    "`````{admonition} Thinking about input data again...\n",
    ":class: tip\n",
    "What would be our `samples` and `features`?\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bfd83",
   "metadata": {},
   "source": [
    "## The data in more depth\n",
    "\n",
    "After getting a first idea of what our `dataset` entails, we should spend a bit more time exploring it in more depth, starting with the neuroimaging files.\n",
    "\n",
    "### Neuroimaging files\n",
    "\n",
    "As seen above, the data includes several `nii` files, which contain `images` of `brain volumes`, either `anatomical` or `functional` `scans`, as well as (`binary`) `masks`. Lets have a look at the `anatomical` `image` first.\n",
    "Using `nilearn`, we can either `load` and then `plot` it or directly `plot` it. Here we are going to do the first option as it will allow us to check the properties of the `image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34510d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dfad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_image = load_img(haxby_dataset.anat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a9627",
   "metadata": {},
   "source": [
    "Now we can access basically all parts of the `image`, including the `header`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b030a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anat_image.header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ac85a",
   "metadata": {},
   "source": [
    "and actual `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f86efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_image.dataobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_image.dataobj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fca86f",
   "metadata": {},
   "source": [
    "As you can see, this is basically a `numpy array` that has the same `dimensions` as our `image` and the `data` reflect `values` for a given `voxel`. So far so good but how does it actually look? We can make use of one of `nilearn`'s many [plotting functions](https://nilearn.github.io/stable/modules/plotting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b37644",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(anat_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0c12e1",
   "metadata": {},
   "source": [
    "We can even create an `interactive plot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3845ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_img(anat_image, symmetric_cmap=False, cmap='Greys_r', colorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221fc35e",
   "metadata": {},
   "source": [
    "Comparably, we can do the same things with the `functional` `image`. That is `load`ing the `image`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a49280",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_image = load_img(haxby_dataset.func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df733c",
   "metadata": {},
   "source": [
    "and inspect its `header`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c008fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(func_image.header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc363de",
   "metadata": {},
   "source": [
    "and `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908fd7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_image.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_image.dataobj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1420c6f",
   "metadata": {},
   "source": [
    "`````{admonition} We already noticed something...\n",
    ":class: tip\n",
    "The `data` of the `anatomical` and `functional` `image` are quite different. Do you know why and which we would use for our planned `decoding` analyses?\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd0bfd1",
   "metadata": {},
   "source": [
    "As we have a `4D` `image`, that is `brain volumes` acquired over time (the `4th dimension`), we need to adapt the `plotting` a bit. More precisely, we need to either `plot` a `3D image` at a given `time point` or e.g. compute the `mean image` over `time` and `plot` that. The latter might be more informative and additional shows you how easy this can be done using `nilearn`'s [image functions](https://nilearn.github.io/stable/modules/image.html). Thus, we, at first, `import` the respective `function` and compute the `mean image`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import mean_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecdc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_image_mean = mean_img(func_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59f20b",
   "metadata": {},
   "source": [
    "We can check if this worked via the approach we followed above, ie checking the `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_image_mean.dataobj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc481f35",
   "metadata": {},
   "source": [
    "That seems about right and we can give the plot a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ec4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_epi(func_image_mean, cmap='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ffe83",
   "metadata": {},
   "source": [
    "and of course, this also works for `interactive` plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66346b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_img(func_image_mean, cmap='magma', symmetric_cmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd7510",
   "metadata": {},
   "source": [
    "The last type of `neuroimaging` file we need to check are the (`binary`) `masks`, so let's do it for one example `mask`: the `ventral temporal cortex`. This mask has been generated as part of the Haxby et al. (2001) study {cite:p}`Haxby2001-vt`, and highlights a part of the brain specialized in the processing of visual information, and which contains areas sensitive to different types of image categories {cite:p}`grill-spector_functional_2014` . As with the types before, we can `load`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_mask = load_img(haxby_dataset.mask_vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add65461",
   "metadata": {},
   "source": [
    "`inspect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24525317",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vt_mask.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b97e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_mask.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_mask.dataobj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d9c45",
   "metadata": {},
   "source": [
    "and `visualize` it (Here, we are going to plot it as an overlay on the `anatomical image`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24500022",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_roi(vt_mask, bg_img=anat_image,\n",
    "                  cmap='Paired')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3313b89",
   "metadata": {},
   "source": [
    "With that, we had a quick look at all `neuroimaging` `file` `types` present in the `dataset` and can continue to have a look at the other `file types` (and information therein) required to apply a `decoding model`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42c220",
   "metadata": {},
   "source": [
    "### Labels and stimulus annotations\n",
    "\n",
    "As mentioned in prior sessions (e.g.[Supervised learning using scikit-learn](https://main-educational.github.io/material.html#supervised-learning-using-scikit-learn) and hinted at the [beginning of this session](#A-short-primer-on-datasets), when working on a `supervised learning problem`, we also need the `ground truth`/`true labels` for each `sample`. Why? Because we need to evaluate how a given `model` performs via comparing the `labels` it `predicted` to the `true labels`. What these `labels` refer to can be manifold and of course depends on the `task` at hand. \n",
    "\n",
    "For example, a `supervised learning problem` in the `dataset` at hand could entail `training` a `model` to `recognize` and `predict` what `category` `participants` perceived based on their `brain activation`. Thus, we would need to know what `category` was shown when during the acquisition of the `data` (or which `category` resulted in which `estimated` `brain activity`). \n",
    "\n",
    "Within our `tutorial dataset`, this information is included in the `session_target` file. Using [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) we can easily `load` and `inspect` this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c14dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stimulus_annotations = pd.read_csv(haxby_dataset.session_target[0], delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f4f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_annotations.head(n=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983f129",
   "metadata": {},
   "source": [
    "While this is already informative, let's plot it to get a better intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.scatterplot(x=stimulus_annotations.index, y=stimulus_annotations['labels'], \n",
    "                     hue=stimulus_annotations['labels'], legend=False, palette='colorblind')\n",
    "plt.title('Categories shown across time')\n",
    "ax.set_xlabel('Time point/fMRI scan')\n",
    "sns.despine(offset=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f4dbc",
   "metadata": {},
   "source": [
    "As we can see, the information provided indicates what `category` `participants` perceived at which `sample` or `fMRI image acquisition` aka point in time during the experiment. With that, we have the needed `labels` for our `samples` (ie our `Y`) and can thus apply a `supervised learning problem`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa073ee",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This already concludes this section of the session within which we explored the went through basic `datasets` concepts again and afterwards explored the `tutorial dataset` which we are going to use during the remaining sections of this session, ie [Decoding via SVM](), [Decoding using MLPs]() and [Decoding using GCNs](). \n",
    "\n",
    "Within this section you should have learned:\n",
    "\n",
    "- important aspects of `datasets`\n",
    "    - structured input in the form `samples X features`\n",
    "    - `small n high p` problem  \n",
    "  \n",
    "  \n",
    "- the `tutorial dataset`\n",
    "    - background\n",
    "    - file types and information therein\n",
    "        - `neuroimaging` files\n",
    "        - `stimulus annotations`\n",
    "\n",
    "If you have any questions, please don't hesitate to ask us. Thank you very much for your attention and see you in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0345bb8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```\n",
    "```\n",
    "\n",
    "+++\n",
    "\n",
    "## Bonus: checking the stimuli\n",
    "\n",
    "As you saw above, our `tutorial dataset` actually also contains the `stimuli` utilized in the experiment. This pretty unique (because of e.g. copyright problems) but really cool. As we could use the `stimuli` for certain analyses, e.g. [encoding]() and/or comparing their processing in `biological` and `artificial neural networks`. However, this is unfortunately outside the scope of this session. Thus, we are just going to plot a few of them so you get an impression.\n",
    "\n",
    "+++\n",
    "\n",
    "We can examine one functional volume using nilearn's plotting tools. Because fmri data are 4D we use [nilearn.image.mean_img](https://nilearn.github.io/modules/generated/nilearn.image.mean_img.html#nilearn.image.mean_img) to extract the average brain volume.\n",
    "\n",
    "```{code-cell} ipython3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nilearn import datasets\n",
    "from nilearn.plotting import show\n",
    "\n",
    "stimulus_information = haxby_dataset.stimuli\n",
    "\n",
    "for stim_type in stimulus_information:\n",
    "  # skip control images, there are too many\n",
    "  if stim_type != 'controls':\n",
    "\n",
    "     file_names = stimulus_information[stim_type]\n",
    "     file_names = file_names[0:16]\n",
    "     fig, axes = plt.subplots(4, 4)\n",
    "     fig.suptitle(stim_type)\n",
    "\n",
    "     for img_path, ax in zip(file_names, axes.ravel()):\n",
    "         ax.imshow(plt.imread(img_path), cmap=plt.cm.gray)\n",
    "\n",
    "     for ax in axes.ravel():\n",
    "         ax.axis(\"off\")\n",
    "\n",
    "show()\n",
    "```\n",
    "\n",
    "Please note that for each `image` `category`, a number of `scrambled images` were also presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2574b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stim_num in range(len(stimulus_information['controls'])):\n",
    "    stim_type = stimulus_information['controls'][stim_num][0]\n",
    "    file_names = stimulus_information['controls'][stim_num][1]  \n",
    "    file_names = file_names[0:16]\n",
    "    fig, axes = plt.subplots(4, 4)\n",
    "    fig.suptitle(stim_type)\n",
    "\n",
    "    for img_path, ax in zip(file_names, axes.ravel()):\n",
    "     ax.imshow(plt.imread(img_path), cmap=plt.cm.gray)\n",
    "\n",
    "    for ax in axes.ravel():\n",
    "     ax.axis(\"off\")\n",
    "\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "main_edu_2022",
   "language": "python",
   "name": "main_edu_2022"
  },
  "source_map": [
   14,
   73,
   81,
   84,
   88,
   91,
   95,
   97,
   108,
   115,
   126,
   130,
   132,
   136,
   138,
   142,
   146,
   148,
   152,
   156,
   158,
   162,
   164,
   168,
   170,
   174,
   176,
   180,
   184,
   186,
   193,
   197,
   201,
   203,
   207,
   209,
   213,
   215,
   219,
   221,
   225,
   227,
   231,
   235,
   239,
   241,
   245,
   248,
   252,
   262,
   267,
   269,
   273,
   282,
   286,
   307,
   354
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}