{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nilearn.datasets\n",
    "import nilearn.connectome\n",
    "import pathlib\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn import datasets, plotting, image\n",
    "from csv import writer\n",
    "\n",
    "sys.path.append(os.path.join(\"..\"))\n",
    "sys.path.append('../src')\n",
    "import gcn_windows_dataset\n",
    "import graph_construction\n",
    "import gcn_model\n",
    "import visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Haxby dataset\n",
    "\n",
    "In this notebook, all the models are trained on the *Haxby* dataset.\n",
    "We could easily removed resting state tasks at the time of selecting data. That would give us higher accuracy results.\n",
    "\n",
    "__*X = masker.fit_transform(func_file)[nonrest_task_mask]* \\\n",
    "*y = labels['labels'][nonrest_task_mask]*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We are fetching the data for subject 4\n",
    "data_dir = os.path.join('..', 'data')\n",
    "sub_no = 4\n",
    "haxby_ds = datasets.fetch_haxby(subjects=[sub_no], fetch_stimuli=True, data_dir=data_dir)\n",
    "\n",
    "func_file = haxby_ds.func[0]\n",
    "\n",
    "# Standardizing\n",
    "mask_vt_file = haxby_ds.mask_vt[0]\n",
    "masker = NiftiMasker(mask_img=mask_vt_file, standardize=True)\n",
    "\n",
    "labels = pd.read_csv(haxby_ds.session_target[0], sep=\" \")\n",
    "\n",
    "# Selecting data\n",
    "X = masker.fit_transform(func_file)\n",
    "y = labels['labels']\n",
    "\n",
    "categories = y.unique()\n",
    "print(categories)\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutional Networks (GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model pipeline:\n",
    "- Takes a short series of fMRI volumes as input.\n",
    "- Maps the fMRI signals onto a predefined brain graph.\n",
    "- Propagates brain dynamics information on inter-connected brain regions & networks.\n",
    "- Generates task-specific representations of recorded brain activities. \n",
    "- Predicts the corresponding task states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"GCN_pipeline.png\" width=850 height=420 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_path = os.path.join(data_dir, 'haxby_proc/')\n",
    "concat_path = os.path.join(data_dir, 'haxby_concat/')\n",
    "conn_path = os.path.join(data_dir, 'haxby_connectomes/')\n",
    "split_path = os.path.join(data_dir, 'haxby_split_win/')\n",
    "    \n",
    "# delete the contents of a folder to avoid inconsistency\n",
    "old_files = glob.glob(concat_path + '/*')\n",
    "for f in old_files:\n",
    "    os.remove(f)    \n",
    "if os.path.exists(split_path):\n",
    "    files = glob.glob(os.path.join(split_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "For the GCN model in order to run the model on different sizes of input, we will concatenate bold data of the same stimuli and save it in a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "old_dirContents = os.listdir(concat_path)\n",
    "print(old_dirContents)\n",
    "\n",
    "concat_bold_files = []\n",
    "if (len(old_dirContents) == 0 or len(old_dirContents) == 1):    \n",
    "    if ((len(X)) == len(y)):\n",
    "        \n",
    "        for i in range(0,len(y)):\n",
    "            label = y[i]\n",
    "            concat_bold_files = X[i:i+1]\n",
    "            concat_file_name = concat_path + '{}_concat_fMRI.npy'.format(label)\n",
    "            file = pathlib.Path(concat_file_name)\n",
    "            \n",
    "            if file.exists ():\n",
    "                concat_file = np.load(concat_file_name, allow_pickle = True)\n",
    "                concat_file = np.concatenate((concat_file, concat_bold_files), axis = 0)\n",
    "                np.save(concat_file_name, concat_file)\n",
    "            else:\n",
    "                np.save(concat_file_name, concat_bold_files)\n",
    "            \n",
    "else:\n",
    "    print('Folder is Not Empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(concat_path + 'phenotypic_data.tsv', 'wt') as out_file:\n",
    "    \n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    tsv_writer.writerow(['label'])\n",
    "    \n",
    "    for category in categories: \n",
    "        tsv_writer.writerow([category])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating connectomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Estimating connectomes\n",
    "corr_measure = nilearn.connectome.ConnectivityMeasure(kind=\"correlation\")\n",
    "conn = corr_measure.fit_transform([X])[0]\n",
    "np.save(os.path.join(conn_path, 'conn_subj{}.npy'.format(sub_no)), conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time windows\n",
    "\n",
    "Different lengths for our input data can be selected. \n",
    "In this example we will continue with *window_length = 1*, which means each input file will have a length equal to just one Repetition Time (TR). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "window_length = 1\n",
    "\n",
    "# Path for saving the files\n",
    "pheno_file = os.path.join(concat_path, 'phenotypic_data.tsv')\n",
    "processed_bold_files = sorted(glob.glob(concat_path + '/*.npy'))\n",
    "conn_files = sorted(glob.glob(conn_path + '/*.npy'))\n",
    "out_file = os.path.join(split_path, '{}_{:04d}.npy')\n",
    "out_csv = os.path.join(split_path, 'labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to split bold input files to the desired windows lenght, then we will also create a csv file that will contain label for each splited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_labels = {'rest':0,'face':1,'chair':2,'scissors':3,'shoe':4,'scrambledpix':5,'house':6,'cat':7,'bottle':8}\n",
    "label_df = pd.DataFrame(columns=['label', 'filename'])\n",
    "\n",
    "for proc_bold in processed_bold_files:\n",
    "    \n",
    "    ts_data = np.load(proc_bold)\n",
    "    ts_duration = len(ts_data)\n",
    "\n",
    "    ts_filename = os.path.basename(proc_bold)\n",
    "    ts_label = ts_filename.split('_', 1)[0]\n",
    "\n",
    "    valid_label = dic_labels[ts_label]\n",
    "    \n",
    "    # Split the timeseries\n",
    "    rem = ts_duration % window_length\n",
    "    n_splits = int(np.floor(ts_duration / window_length))\n",
    "\n",
    "    ts_data = ts_data[:(ts_duration-rem), :]   \n",
    "    \n",
    "    for j, split_ts in enumerate(np.split(ts_data, n_splits)):\n",
    "        ts_output_file_name = out_file.format(ts_filename, j)\n",
    "\n",
    "        split_ts = np.swapaxes(split_ts, 0, 1)\n",
    "        np.save(ts_output_file_name, split_ts)\n",
    "        curr_label = {'label': valid_label, 'filename': os.path.basename(ts_output_file_name)}\n",
    "        label_df = label_df.append(curr_label, ignore_index=True)\n",
    "    \n",
    "label_df.to_csv(out_csv, index=False)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset\n",
    "\n",
    "Here we will split the processed data into three diferent sets for train, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "\n",
    "train_dataset = gcn_windows_dataset.TimeWindowsDataset(\n",
    "    data_dir=split_path\n",
    "    , partition=\"train\"\n",
    "    , random_seed=random_seed\n",
    "    , pin_memory=True\n",
    "    , normalize=True,\n",
    "    shuffle = True)\n",
    "\n",
    "valid_dataset = gcn_windows_dataset.TimeWindowsDataset(\n",
    "    data_dir=split_path\n",
    "    , partition=\"valid\"\n",
    "    , random_seed=random_seed\n",
    "    , pin_memory=True\n",
    "    , normalize=True, \n",
    "    shuffle = True)\n",
    "\n",
    "test_dataset = gcn_windows_dataset.TimeWindowsDataset(\n",
    "    data_dir=split_path\n",
    "    , partition=\"test\"\n",
    "    , random_seed=random_seed\n",
    "    , pin_memory=True\n",
    "    , normalize=True, \n",
    "    shuffle = True)\n",
    "\n",
    "print(\"train dataset: {}\".format(train_dataset))\n",
    "print(\"valid dataset: {}\".format(valid_dataset))\n",
    "print(\"test dataset: {}\".format(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "train_generator = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=True)\n",
    "test_generator = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "train_features, train_labels = next(iter(train_generator))\n",
    "print(f\"Feature batch shape: {train_features.size()}; mean {torch.mean(train_features)}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}; mean {torch.mean(torch.Tensor.float(train_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting connectomes\n",
    "We will get the average connectome with its k-nearest neighbors, in this model *k = 8* is considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectomes = []\n",
    "for conn_file in conn_files:\n",
    "      connectomes += [np.load(conn_file)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __6__ graph convolutional layers with __32 graph filters__  at each layer\n",
    "- followed by a __global average pooling__ layer & __2 fully connected__ layers \n",
    "- k-NN graph was made using __8 nodes__ with the highest connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph = graph_construction.make_group_graph(connectomes, self_loops=False, \n",
    "                                            k=8, symmetric=True)\n",
    "# Create model\n",
    "gcn = gcn_model.GCN(graph.edge_index, graph.edge_attr, \n",
    "                           n_timepoints=window_length)\n",
    "gcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)    \n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss, current = loss.item(), batch * dataloader.batch_size #Loic\n",
    "\n",
    "        correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        correct /= X.shape[0]\n",
    "        print(f\"#{batch:>5};\\ttrain_loss: {loss:>0.3f};\\ttrain_accuracy:{(100*correct):>5.1f}%\\t\\t[{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def valid_test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model.forward(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=1e-4, weight_decay=5e-4)\n",
    "\n",
    "epochs = 15\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}/{epochs}\\n-------------------------------\")\n",
    "    train_loop(train_generator, gcn, loss_fn, optimizer)\n",
    "    loss, correct = valid_test_loop(valid_generator, gcn, loss_fn)\n",
    "    print(f\"Valid metrics:\\n\\t avg_loss: {loss:>8f};\\t avg_accuracy: {(100*correct):>0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rasults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss, correct = valid_test_loop(test_generator, gcn, loss_fn) \n",
    "print(f\"Test metrics:\\n\\t avg_loss: {loss:>f};\\t avg_accuracy: {(100*correct):>0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multilayer perceptron (MLP)\n",
    "\n",
    "- with 2 dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from nilearn.plotting import plot_anat, show, plot_stat_map, plot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the string to numerical values. (ML Algorithm can only work on numbers and not on string)\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "\n",
    "# reshapeing y\n",
    "temp = np.reshape(y, (len(y),1))\n",
    "y = temp\n",
    "\n",
    "# creating instance of one-hot-encoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# passing bridge-types-cat column (label encoded values of bridge_types)\n",
    "y = pd.DataFrame(enc.fit_transform(y).toarray())\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset\n",
    "We will shuffle and split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 0)\n",
    "\n",
    "#standarize features caling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique conditions that we have\n",
    "mlp_classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "mlp_classifier.add(Dense(338 , input_dim = 675, kernel_initializer=\"uniform\", activation = 'relu'))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "mlp_classifier.add(Dense(169, kernel_initializer=\"uniform\", activation = 'relu'))\n",
    "\n",
    "# Using softmax at the end, lenght of categories shows the number of labels we have\n",
    "mlp_classifier.add(Dense(len(categories), activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "mlp_classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model on the Training set\n",
    "history = mlp_classifier.fit(X_train, y_train, batch_size = 10,\n",
    "                             epochs = 10, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history = visualization.classifier_history (history, 'MLP ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the predictions and evaluating the model\n",
    "y_pred = mlp_classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print ('mean accuracy score:', np.round(accuracy_score(y_test.values.argmax(axis = 1), \n",
    "                                                       y_pred.argmax(axis=1), normalize = True, \n",
    "                                                       sample_weight=None),2))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_mlp = confusion_matrix(y_test.values.argmax(axis = 1), y_pred.argmax(axis=1))\n",
    "model_conf_matrix = cm_mlp.astype('float') / cm_mlp.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "visualization.conf_matrix(model_conf_matrix, \n",
    "                          categories, \n",
    "                          title='MLP decoding results on Haxby')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "hcptrtr_gcn_env",
   "language": "python",
   "name": "hcptrtr_gcn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
